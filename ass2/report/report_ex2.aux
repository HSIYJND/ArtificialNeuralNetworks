\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}`Pure' Bayesian training versus `trainbr'}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Prior, posterior and classification with corresponding separation line of the original (blue) and MAP network (red). Some (but not many) samples are misclassified.}}{1}}
\newlabel{fig:example_perbayes}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian regularization using `trainbr'}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of an overfitted network trained with the Levenberg-Marquadt algorithm (red), compared to the results of the Bayesian learning (green) for a data set with $h(x)=\qopname  \relax o{sin}(x) + \mathcal  {N}(\sigma )$ where $\sigma = 0.3$. Both networks have the same architecture, which includes $10$ neurons in the hidden layer.}}{2}}
\newlabel{fig:overfitting_example}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bias and variance plot as a function of the complexity of the neural network.}}{2}}
\newlabel{fig:bias_and_variance_plot}{{3}{2}}
